---
title: "Data Manipulation Challenge"
subtitle: "A Mental Model for Method Chaining in Pandas"
format:
  html: default
 # pdf: default
execute:
  echo: true
  eval: true
---

# ðŸ”— Data Manipulation Challenge - A Mental Model for Method Chaining in Pandas

::: {.callout-important}
## ðŸ“Š Challenge Requirements In Section [Student Analysis Section](#student-analysis-section)
- Complete all discussion questions for the seven mental models (plus some extra requirements for higher grades)
:::

::: {.callout-important}
## ðŸŽ¯ Note on Python Usage

**Recommended Workflow: Use Your Existing Virtual Environment**
If you completed the Tech Setup Challenge Part 2, you already have a virtual environment set up! Here's how to use it for this new challenge:

1. **Clone this new challenge repository** (see Getting Started section below)
2. **Open the cloned repository in Cursor**
3. **Set this project to use your existing Python interpreter:**
   - Press `Ctrl+Shift+P` â†’ "Python: Select Interpreter"
   - Navigate to and choose the interpreter from your existing virtual environment (e.g., `your-previous-project/venv/Scripts/python.exe`)
4. **Activate the environment in your terminal:**
   - Open terminal in Cursor (`Ctrl + ``)
   - Navigate to your previous project folder where you have the `venv` folder
   - **ðŸ’¡ Pro tip:** You can quickly navigate by typing `cd` followed by dragging the folder from your file explorer into the terminal
   - Activate using the appropriate command for your system:
     - **Windows Command Prompt:** `venv\Scripts\activate`
     - **Windows PowerShell:** `.\venv\Scripts\Activate.ps1`
     - **Mac/Linux:** `source venv/bin/activate`
   - You should see `(venv)` at the beginning of your terminal prompt
5. **Install additional packages if needed:** `pip install pandas numpy matplotlib seaborn`

::: {.callout-warning}
## âš ï¸ Cloud Storage Warning

**Avoid using Google Drive, OneDrive, or other cloud storage for Python projects!** These services can cause issues with:
- Package installations failing due to file locking
- Virtual environment corruption
- Slow performance during pip operations

**Best practice:** Keep your Python projects in a local folder like `C:\Users\YourName\Documents\` or `~/Documents/` instead of cloud-synced folders.
:::

**Alternative: Create a New Virtual Environment**
If you prefer a fresh environment, follow the Quarto documentation: [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html). Be sure to follow the instructions to activate the environment, set it up as your default Python interpreter for the project, and install the necessary packages (e.g. pandas) for this challenge.  For installing the packages, you can use the `pip install -r requirements.txt` command since you already have the requirements.txt file in your project.   Some steps do take a bit of time, so be patient.

**Why This Works:** Virtual environments are portable - you can use the same environment across multiple projects, and Cursor automatically activates it when you select the interpreter!

:::

## The Problem: Mastering Data Manipulation Through Method Chaining

**Core Question:** How can we efficiently manipulate datasets using `pandas` method chaining to answer complex business questions?

**The Challenge:** Real-world data analysis requires combining multiple data manipulation techniques in sequence. Rather than creating intermediate variables at each step, method chaining allows us to write clean, readable code that flows logically from one operation to the next.

**Our Approach:** We'll work with ZappTech's shipment data to answer critical business questions about service levels and cross-category orders, using the seven mental models of data manipulation through pandas method chaining.

::: {.callout-warning}
## âš ï¸ AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance â†’ Awareness â†’ Learning â†’ Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Seven Mental Models of Data Manipulation

The seven most important ways we manipulate datasets are:

1. **Assign:** Add new variables with calculations and transformations
2. **Subset:** Filter data based on conditions or select specific columns
3. **Drop:** Remove unwanted variables or observations
4. **Sort:** Arrange data by values or indices
5. **Aggregate:** Summarize data using functions like mean, sum, count
6. **Merge:** Combine information from multiple datasets
7. **Split-Apply-Combine:** Group data and apply functions within groups


## Data and Business Context

We analyze ZappTech's shipment data, which contains information about product deliveries across multiple categories. This dataset is ideal for our analysis because:

- **Real Business Questions:** CEO wants to understand service levels and cross-category shopping patterns
- **Multiple Data Sources:** Requires merging shipment data with product category information
- **Complex Relationships:** Service levels may vary by product category, and customers may order across categories
- **Method Chaining Practice:** Perfect for demonstrating all seven mental models in sequence

## Data Loading and Initial Exploration

Let's start by loading the ZappTech shipment data and understanding what we're working with.

```{python}
#| label: load-data
#| echo: true
#| message: false
#| warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Load the shipment data
shipments_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/shipments.csv", 
    parse_dates=['plannedShipDate', 'actualShipDate']
)

# Load product line data
product_line_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/productLine.csv"
)

# Reduce dataset size for faster processing (4,000 rows instead of 96,805 rows)
shipments_df = shipments_df.head(4000)

print("Shipments data shape:", shipments_df.shape)
print("\nShipments data columns:", shipments_df.columns.tolist())
print("\nFirst few rows of shipments data:")
print(shipments_df.head(10))

print("\n" + "="*50)
print("Product line data shape:", product_line_df.shape)
print("\nProduct line data columns:", product_line_df.columns.tolist())
print("\nFirst few rows of product line data:")
print(product_line_df.head(10))
```

::: {.callout-note}
## ðŸ’¡ Understanding the Data

**Shipments Data:** Contains individual line items for each shipment, including:
- `shipID`: Unique identifier for each shipment
- `partID`: Product identifier
- `plannedShipDate`: When the shipment was supposed to go out
- `actualShipDate`: When it actually shipped
- `quantity`: How many units were shipped

**Product Category and Line Data:** Contains product category information:
- `partID`: Links to shipments data
- `productLine`: The category each product belongs to
- `prodCategory`: The category each product belongs to

**Business Questions We'll Answer:**
1. Does service level (on-time shipments) vary across product categories?
2. How often do orders include products from more than one category?
:::

## The Seven Mental Models: A Progressive Learning Journey

Now we'll work through each of the seven mental models using method chaining, starting simple and building complexity.

### 1. Assign: Adding New Variables

**Mental Model:** Create new columns with calculations and transformations.

Let's start by calculating whether each shipment was late:

```{python}
#| label: mental-model-1-assign
#| echo: true

shipments_with_lateness = (
    shipments_df
    .assign(
        is_late   = lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late = lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        lateStatement = lambda df: np.where(df["is_late"], "Darn Shipment is Late", "Shipment is on Time")
    )
)
shipments_with_lateness[['shipID','plannedShipDate','actualShipDate','is_late','days_late','lateStatement']].head()
```


::: {.callout-tip}
## ðŸ’¡ Method Chaining Tip for New Python Users

**Why use `lambda df:`?** When chaining methods, we need to reference the current state of the dataframe. The `lambda df:` tells pandas "use the current dataframe in this calculation." Without it, pandas would look for a variable called `df` that doesn't exist.

**Alternative approach:** You could also write this as separate steps, but method chaining keeps related operations together and makes the code more readable.
:::

::: {.callout-important}
## ðŸ¤” Discussion Questions: Assign Mental Model

**Question 1: Data Types and Date Handling**
- What is the `dtype` of the `actualShipDate` series? How can you find out using code?
- Why is it important that both `actualShipDate` and `plannedShipDate` have the same data type for comparison?

**Question 2: String vs Date Comparison**
- Can you give an example where comparing two dates as strings would yield unintuitive results, e.g. what happens if you try to compare "04-11-2025" and "05-20-2024" as strings vs as dates?

**Question 3: Debug This Code**
```python
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        lateStatement="Darn Shipment is Late" if shipments_df['is_late'] else "Shipment is on Time"
    )
)
```
What's wrong with the `lateStatement` assignment and how would you fix it?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

### ðŸ§© Rohit's answers will be here- Assign Mental Model â€” Discussion Answers

**Q1: Data Types and Date Handling**

```python
shipments_df["actualShipDate"].dtype
```
Both `actualShipDate` and `plannedShipDate` must be date (`datetime64[ns]`) types.  
If one is text, pandas compares letters instead of real time, giving wrong results.

---

**Q2: String vs Date Comparison**

```python
"04-11-2025" > "05-20-2024"      # compares text
pd.to_datetime("04-11-2025") > pd.to_datetime("05-20-2024")   # compares real dates
```
String compare looks at letters; datetime compare uses actual dates.  
Always convert with `pd.to_datetime()` before comparing.

---

**Q3: Debug This Code**

Wrong:
```python
lateStatement = "Darn Shipment is Late" if shipments_df['is_late'] else "Shipment is on Time"
```
It tries one `if` on a whole column.  
âœ… Fix:
```python
lateStatement = lambda d: np.where(d["is_late"], "Darn Shipment is Late", "Shipment is on Time")
```
`np.where()` checks each row separately.



### 2. Subset: Querying Rows and Filtering Columns

**Mental Model:** Query rows based on conditions and filter to keep specific columns.

Let's query for only late shipments and filter to keep the columns we need:

```{python}
#| label: mental-model-2-subset
#| echo: true

# 0) sanity check
print("assign ready:", "shipments_with_lateness" in globals())

# 1) keep only late rows and selected columns
late_shipments = (
    shipments_with_lateness
    .query("is_late == True")
    .loc[:, ["shipID","partID","plannedShipDate","actualShipDate","days_late"]]
)

# 2) use a variable in query (example: 5+ days late)
late_threshold = 5
late_5plus = (
    shipments_with_lateness
    .query("days_late >= @late_threshold")
    .loc[:, ["shipID","partID","days_late"]]
)

print("Late shipments:", len(late_shipments))
from IPython.display import display
display(late_shipments.head(3))
print("\n5+ days late sample:")
display(late_5plus.head(5))
```



::: {.callout-note}
## ðŸ” Understanding the Methods

- **`.query()`**: Query rows based on conditions (like SQL WHERE clause)
- **`.filter()`**: Filter to keep specific columns by name
- **Alternative**: You could use `.loc[]` for more complex row querying, but `.query()` is often more readable
:::

::: {.callout-important}
## ðŸ¤” Discussion Questions: Subset Mental Model

**Question 1: Query vs Boolean Indexing**
- What's the difference between using `.query('is_late == True')` and `[df['is_late'] == True]`?
- Which approach is more readable and why?

**Question 2: Additional Row Querying**
- Can you show an example of using a variable like `late_threshold` to query rows for shipments that are at least `late_threshold` days late, e.g. what if you wanted to query rows for shipments that are at least 5 days late?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

#### ðŸ§© Rohit's answers will be here- Subset Mental Model â€” Discussion Answers
**ðŸ§© Subset Mental Model â€” Discussion Answers**

**Q1: Query vs Boolean Indexing**  
Using `.query('is_late == True')` is more readable and shorter than using `df[df["is_late"] == True]`.  
Both give the same result, but `.query()` looks cleaner and easier to write when filtering by column names.

**Q2: Using a Variable Inside Query**  
We can filter rows using a variable, like this:  
```python
late_threshold = 5
late_5plus = shipments_with_lateness.query("days_late >= @late_threshold")
```
The `@` lets pandas know to use the variable `late_threshold` inside the query.  
This keeps code flexible â€” we can easily change the number (like 3 or 10) without editing the whole line.


### 3. Drop: Removing Unwanted Data

**Mental Model:** Remove columns or rows you don't need.

Let's clean up our data by removing unnecessary columns:

```{python}
#| label: mental-model-3-drop
#| echo: true

# (safety guard) rebuild prior step if kernel was restarted
if "shipments_with_lateness" not in globals():
    shipments_with_lateness = (
        shipments_df
        .assign(
            is_late   = lambda d: d["actualShipDate"] > d["plannedShipDate"],
            days_late = lambda d: (d["actualShipDate"] - d["plannedShipDate"]).dt.days
        )
    )

# 1) drop columns we don't need for this analysis (quantity is optional in this dataset)
clean_shipments = (
    shipments_with_lateness
    .drop(columns=["quantity"], errors="ignore")         # ignore if column doesn't exist
    .dropna(subset=["plannedShipDate","actualShipDate"]) # keep only rows with both dates
)

# tiny audit so we see what changed
rows_before = len(shipments_with_lateness)
rows_after  = len(clean_shipments)
print({"rows_before": rows_before, "rows_after": rows_after, "dropped_rows": rows_before - rows_after})
print("Columns kept:", list(clean_shipments.columns))
clean_shipments.head(3)
```


::: {.callout-important}
## ðŸ¤” Discussion Questions: Drop Mental Model

**Question 1: Drop vs Filter Strategies**
- What's the difference between `.drop(columns=['quantity'])` and `.filter()` with a list of columns you want to keep?
- When would you choose to drop columns vs filter to keep specific columns?

**Question 2: Handling Missing Data**
- What happens if you use `.dropna()` without specifying `subset`? How is this different from `.dropna(subset=['plannedShipDate', 'actualShipDate'])`?
- Why might you want to be selective about which columns to check for missing values?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

#### ðŸ§© Rohit's answers will be here- Drop Mental Model â€” Discussion Answers

**ðŸ§© Drop Mental Model â€” Discussion Answers**

**Q1: Drop vs Filter (keep specific columns)**  
- `.drop(columns=[...])` = remove a few columns you donâ€™t need.  
- `.loc[:, keep_list]` = keep **only** a whitelist of columns.  
Pick `.drop` when you know the unwanted columns; pick `.loc` when you want a tight, known set to keep.

**Q2: Handling Missing Data**
`dropna()` vs `dropna(subset=...)`**  
- `.dropna()` removes any row that has **any** missing value in **any** column.  
- `.dropna(subset=["plannedShipDate","actualShipDate"])` only checks those columns.  
We use `subset` here because we only require the two date fields to compute lateness.


### 4. Sort: Arranging Data

**Mental Model:** Order data by values or indices.

Let's sort by lateness to see the worst offenders:

```{python}
#| label: mental-model-4-sort
#| echo: true

# safety: rebuild prior result if kernel was restarted
if "clean_shipments" not in globals():
    # re-create from earlier steps
    shipments_with_lateness = (
        shipments_df
        .assign(
            is_late   = lambda d: d["actualShipDate"] > d["plannedShipDate"],
            days_late = lambda d: (d["actualShipDate"] - d["plannedShipDate"]).dt.days
        )
    )
    clean_shipments = (
        shipments_with_lateness
        .drop(columns=["quantity"], errors="ignore")
        .dropna(subset=["plannedShipDate","actualShipDate"])
    )

# 1) sort by days_late (largest first) and reset row numbers
sorted_by_lateness = (
    clean_shipments
    .sort_values("days_late", ascending=False)   # biggest delay at top
    .reset_index(drop=True)                      # make row numbers 0..N-1
)

# 2) example: sort by two columns (late first, then size of delay)
sorted_two_keys = (
    clean_shipments
    .sort_values(["is_late", "days_late"], ascending=[False, False])
    .reset_index(drop=True)
)

print("Top 8 worst delays:")
sorted_by_lateness.loc[:, ["shipID","partID","days_late","is_late"]].head(8)
```


::: {.callout-important}
## ðŸ¤” Discussion Questions: Sort Mental Model

**Question 1: Sorting Strategies**
- What's the difference between `ascending=False` and `ascending=True` in sorting?
- How would you sort by multiple columns (e.g., first by `is_late`, then by `days_late`)?

**Question 2: Index Management**
- Why do we use `.reset_index(drop=True)` after sorting?
- What happens to the original index when you sort? Why might this be problematic?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

#### ðŸ§© Rohit's answers will be here- Sort Mental Model â€” Discussion Answers
**ðŸ§© Sort Mental Model â€” Discussion Answers**   

**Q1: Sorting options**  
- `ascending=False` â†’ largest values first.  
- Sort by multiple columns:
  ```python
  df.sort_values(["is_late","days_late"], ascending=[False, False])
  ```

**Q2: Why reset the index?**  
Sorting keeps the **old** row numbers, which can look messy.  
`.reset_index(drop=True)` creates clean 0..N-1 row numbers after sorting.
s

### 5. Aggregate: Summarizing Data

**Mental Model:** Calculate summary statistics across groups or the entire dataset.

Let's calculate overall service level metrics:

```{python}
#| label: mental-model-5-aggregate
#| echo: true

# safety: rebuild if needed (kernel was restarted)
if "clean_shipments" not in globals():
    shipments_with_lateness = (
        shipments_df
        .assign(
            is_late   = lambda d: d["actualShipDate"] > d["plannedShipDate"],
            days_late = lambda d: (d["actualShipDate"] - d["plannedShipDate"]).dt.days
        )
    )
    clean_shipments = (
        shipments_with_lateness
        .drop(columns=["quantity"], errors="ignore")
        .dropna(subset=["plannedShipDate","actualShipDate"])
    )

# --- Aggregate: one-row summary of service metrics (named aggregation) ---
service_metrics = clean_shipments.agg(
    shipments  = ("is_late", "size"),  # total rows
    late_count = ("is_late", "sum"),   # count of True
    late_rate  = ("is_late", "mean"),  # percent True
    avg_late   = ("days_late", "mean"),
    max_late   = ("days_late", "max"),
)

# Ensure we always end up with a 1-row DataFrame (Series -> DataFrame)
import pandas as pd
if isinstance(service_metrics, pd.Series):
    service_metrics = service_metrics.to_frame().T

# Round only for display
service_metrics_display = service_metrics.round({"late_rate": 3, "avg_late": 2})
service_metrics_display
```



::: {.callout-important}
## ðŸ¤” Discussion Questions: Aggregate Mental Model

**Question 1: Boolean Aggregation**
- Why does `sum()` work on boolean values? What does it count?

:::


#### Briefly Give Answers to the Discussion Questions In This Section

**ðŸ§© Aggregate Mental Model â€” Discussion Answers**

**Q1: Why does `sum()` work on booleans?**  
`True` counts as `1` and `False` as `0`. So `sum()` counts Trues, and `mean()` gives the fraction of Trues.

**Q2: Why round only at the end?**  
Rounding early can change the math. Keep full precision for calculations, then round only for display.


### 6. Merge: Combining Information

**Mental Model:** Join data from multiple sources to create richer datasets.

Now let's analyze service levels by product category. First, we need to merge our data:

```{python}
#| label: mental-model-6-merge
#| echo: true

# safety: rebuild prior results if needed
if "clean_shipments" not in globals():
    shipments_with_lateness = (
        shipments_df
        .assign(
            is_late   = lambda d: d["actualShipDate"] > d["plannedShipDate"],
            days_late = lambda d: (d["actualShipDate"] - d["plannedShipDate"]).dt.days
        )
    )
    clean_shipments = (
        shipments_with_lateness
        .drop(columns=["quantity"], errors="ignore")
        .dropna(subset=["plannedShipDate","actualShipDate"])
    )

# --- Merge with product info (keep all shipments) ---
rows_before = len(clean_shipments)

# Prefer validation (many shipments -> one product row). If duplicates exist, skip validation gracefully.
try:
    shipments_with_category = clean_shipments.merge(
        product_line_df,
        on="partID",
        how="left",
        validate="many_to_one"  # guard against duplicate partID in product table
    )
except Exception as e:
    print("Note: validate='many_to_one' failed, merging without validate. Details:", e)
    shipments_with_category = clean_shipments.merge(
        product_line_df,
        on="partID",
        how="left"
    )

rows_after = len(shipments_with_category)

# add a helper flag (late + has category info)
shipments_with_category = shipments_with_category.assign(
    category_late = lambda d: d["is_late"] & d["prodCategory"].notna()
)

# small join audit
missing_cat = shipments_with_category["prodCategory"].isna().sum()
print({
    "rows_before_merge": rows_before,
    "rows_after_merge": rows_after,
    "lost_rows": rows_before - rows_after,
    "missing_prodCategory": int(missing_cat)
})

# quick peek
from IPython.display import display
display(
    shipments_with_category
      .loc[:, ["shipID","partID","prodCategory","is_late","days_late","category_late"]]
      .head(8)
)

# category coverage
print("\nCategory counts (non-missing):")
display(shipments_with_category["prodCategory"].value_counts().head(10))
```


::: {.callout-important}
## ðŸ¤” Discussion Questions: Merge Mental Model

**Question 1: Join Types and Data Loss**
- Why does your professor think we should use `how='left'` in most cases? 
- How can you check if any shipments were lost during the merge?

**Question 2: Key Column Matching**
- What happens if there are duplicate `partID` values in the `product_line_df`?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

#### ðŸ§© Rohit's answers will be here- Merge Mental Model â€” Discussion Answers

**ðŸ§© Merge Mental Model â€” Discussion Answers**

**Q1: Why use `how="left"`?**  
We want to keep **all shipments** even if some parts donâ€™t have category info yet. A left join keeps the shipment rows.

**Q2: How to check if we lost rows in the merge?**  
Compare counts **before vs after** the merge. They should match for a left join:
```python
len(clean_shipments), len(shipments_with_category)
```

**Q3: What if `partID` has duplicates in the product table?**  
Duplicates create many-to-many matches (rows can multiply). Use
`validate="many_to_one"` to catch it. If it fails, fix the product table so
each `partID` maps to **one** category.


### 7. Split-Apply-Combine: Group Analysis

**Mental Model:** Group data and apply functions within each group.

Now let's analyze service levels by category:

```{python}
#| label: mental-model-7-groupby
#| echo: true

# safety: rebuild merge result if needed
if "shipments_with_category" not in globals():
    # rebuild from earlier steps
    shipments_with_lateness = (
        shipments_df
        .assign(
            is_late   = lambda d: d["actualShipDate"] > d["plannedShipDate"],
            days_late = lambda d: (d["actualShipDate"] - d["plannedShipDate"]).dt.days
        )
    )
    clean_shipments = (
        shipments_with_lateness
        .drop(columns=["quantity"], errors="ignore")
        .dropna(subset=["plannedShipDate","actualShipDate"])
    )
    shipments_with_category = clean_shipments.merge(
        product_line_df, on="partID", how="left"
    ).assign(category_late = lambda d: d["is_late"] & d["prodCategory"].notna())

# --- Group by product category: late rate + lateness stats ---
service_by_category = (
    shipments_with_category
    .groupby("prodCategory", as_index=False)
    .agg(
        shipments = ("is_late", "size"),
        late      = ("is_late", "sum"),
        late_rate = ("is_late", "mean"),
        avg_late  = ("days_late", "mean"),
        max_late  = ("days_late", "max")
    )
    .assign(on_time_rate = lambda d: 1 - d["late_rate"])
    .round({"late_rate": 3, "on_time_rate": 3, "avg_late": 2})
    .sort_values("on_time_rate", ascending=False)
)

from IPython.display import display
display(service_by_category.head(10))

# --- (optional) multi-level grouping example: by ship + category ---
ship_cat_summary = (
    shipments_with_category
    .groupby(["shipID","prodCategory"], as_index=False)
    .agg(
        late_any  = ("is_late","any"),
        max_late  = ("days_late","max")
    )
)
display(ship_cat_summary.head(8))
```


::: {.callout-important}
## ðŸ¤” Discussion Questions: Split-Apply-Combine Mental Model

**Question 1: GroupBy Mechanics**
- What does `.groupby('prodCategory')` actually do? How does it "split" the data?
- Why do we need to use `.agg()` after grouping? What happens if you don't?

**Question 2: Multi-Level Grouping**
- Explore grouping by `['shipID', 'prodCategory']`?  What question does this answer versus grouping by `'prodCategory'` alone?  (HINT: There may be many rows with identical shipID's due to a particular order having multiple partID's.)
:::

#### Briefly Give Answers to the Discussion Questions In This Section

#### ðŸ§© Rohit's answers will be here- Split-Apply-Combine Mental Model â€” Discussion Answers

**ðŸ§© Splitâ€“Applyâ€“Combine Mental Model â€” Discussion Answers**

**Q1: What does `groupby('prodCategory')` do?**  
It **splits** the data into one group per category.  
Then `.agg(...)` **applies** functions inside each group and **combines** the results into a summary table.

**Q2: Why do we need `.agg()` after `groupby`?**  
`groupby` just makes the groups; `.agg()` tells pandas **what to calculate** for each group (counts, mean, max, etc.).

**Q3: Whatâ€™s different about grouping by `['shipID','prodCategory']`?**  
It summarizes **within each shipment** for each category (for example, was **any** item late in that shipment/category?).  
Grouping by just `'prodCategory'` ignores shipment boundaries and gives overall category stats.


## Answering A Business Question

**Mental Model:** Combine multiple data manipulation techniques to answer complex business questions.

Let's create a comprehensive analysis by combining shipment-level data with category information:

```{python}
#| label: business-question
#| echo: true

# Q: Do certain categories have better service? How many multi-category shipments?

# safety: rebuild if needed
if "shipments_with_category" not in globals():
    shipments_with_lateness = (
        shipments_df
        .assign(
            is_late   = lambda d: d["actualShipDate"] > d["plannedShipDate"],
            days_late = lambda d: (d["actualShipDate"] - d["plannedShipDate"]).dt.days
        )
    )
    clean_shipments = (
        shipments_with_lateness
        .drop(columns=["quantity"], errors="ignore")
        .dropna(subset=["plannedShipDate","actualShipDate"])
    )
    shipments_with_category = clean_shipments.merge(
        product_line_df, on="partID", how="left"
    )

# 1) service by category (on-time rate)
service_by_category = (
    shipments_with_category
    .groupby("prodCategory", as_index=False)
    .agg(
        shipments = ("is_late","size"),
        late      = ("is_late","sum"),
        late_rate = ("is_late","mean")
    )
    .assign(on_time_rate = lambda d: 1 - d["late_rate"])
    .sort_values("on_time_rate", ascending=False)
)

# 2) multi-category shipments (% of shipments that include >1 category)
ship_cat = (
    shipments_with_category
    .groupby(["shipID","prodCategory"], as_index=False)
    .size()
)
multi_cat = (
    ship_cat.groupby("shipID")["prodCategory"].nunique().rename("n_cat").reset_index()
)
multi_share = (multi_cat["n_cat"] > 1).mean() * 100

# Display quick results
from IPython.display import display
print(f"Share of shipments with multiple product categories: {multi_share:.1f}%")
display(service_by_category.head(10))
```


::: {.callout-important}
## ðŸ¤” Discussion Questions: Answering A Business Question

**Question 1: Business Question Analysis**
- What business question does this comprehensive analysis answer?
- How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?
- What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

#### ðŸ§© Rohit's answers will be here- Answering A Business Question â€” Discussion Answers    

**ðŸ§© Business Question â€” Discussion Answers**

**Q1: What are we answering?**  
Weâ€™re showing **which categories are most on-time** (highest on-time %) and **how many orders span multiple categories**.

**Q2: Why group by `['shipID','prodCategory']` for multi-category?**  
It counts unique categories **within each shipment**, so we can tell if a single order had parts from multiple categories.

**Q3: What can management do with this?**  
Focus on categories with lower on-time %, and plan staffing/inventory where multi-category orders are common.


## Student Analysis Section: Mastering Data Manipulation {#student-analysis-section}

**Your Task:** Demonstrate your mastery of the seven mental models through comprehensive discussion and analysis. The bulk of your grade comes from thoughtfully answering the discussion questions for each mental model. See below for more details.

### Core Challenge: Discussion Questions Analysis

**For each mental model, provide:**
- Clear, concise answers to all discussion questions
- Code examples where appropriate to support your explanations

::: {.callout-important}
## ðŸ“Š Discussion Questions Requirements

**Complete all discussion question sections:**
1. **Assign Mental Model:** Data types, date handling, and debugging
2. **Subset Mental Model:** Filtering strategies and complex queries
3. **Drop Mental Model:** Data cleaning and quality management
4. **Sort Mental Model:** Data organization and business logic
5. **Aggregate Mental Model:** Summary statistics and business metrics
6. **Merge Mental Model:** Data integration and quality control
7. **Split-Apply-Combine Mental Model:** Group analysis and advanced operations
8. **Answering A Business Question:** Combining multiple data manipulation techniques to answer a business question
:::

### Professional Visualizations (For 100% Grade)

**Your Task:** Create a professional visualization that supports your analysis and demonstrates your understanding of the data.

**Create visualizations showing:**
- Service level (on-time percentage) by product category

**Your visualizations should:**
- Use clear labels and professional formatting
- Support the insights from your discussion questions
- Be appropriate for a business audience
- Do not `echo` the code that creates the visualizations

### Professional Visualizations
```{python}
#| label: viz-on-time-rate-by-category
#| echo: false
#| fig-cap: "On-time percentage by product category"
#| fig-width: 9
#| fig-height: 3.8
#| dpi: 150
#| fig-align: center

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

# make a clean copy sorted by on-time rate (ascending for barh)
_plot = (
    service_by_category
    .dropna(subset=["on_time_rate"])
    .sort_values("on_time_rate", ascending=True)
    .assign(on_time_pct=lambda d: (d["on_time_rate"] * 100).round(1))
)

fig, ax = plt.subplots()
ax.barh(_plot["prodCategory"].astype(str), _plot["on_time_pct"])

# x axis in 0â€“100% with percent labels
ax.set_xlim(0, 100)
ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=100))

# labels & title
ax.set_xlabel("On-time %")
ax.set_ylabel("Product Category")
ax.set_title("On-time Delivery by Category")

# light grid and cleaner frame
ax.grid(axis="x", linestyle=":", alpha=0.5)
for spine in ["top", "right"]:
    ax.spines[spine].set_visible(False)

# value labels at the end of bars
for i, v in enumerate(_plot["on_time_pct"]):
    ax.text(v + 0.6, i, f"{v:.1f}%", va="center")

plt.tight_layout()
```



## Challenge Requirements ðŸ“‹

**Your Primary Task:** Answer all discussion questions for the seven mental models with thoughtful, well-reasoned responses that demonstrate your understanding of data manipulation concepts.

**Key Requirements:**
- Complete discussion questions for each mental model
- Demonstrate clear understanding of pandas concepts and data manipulation techniques
- Write clear, business-focused analysis that explains your findings

## Getting Started: Repository Setup ðŸš€

::: {.callout-important}
## ðŸ“ Getting Started

**Step 1:** Fork and clone this challenge repository
- Go to the course repository and find the "dataManipulationChallenge" folder
- Fork it to your GitHub account, or clone it directly
- Open the cloned repository in Cursor

**Step 2:** Set up your Python environment
- Follow the Python setup instructions above (use your existing venv from Tech Setup Challenge Part 2)
- Make sure your virtual environment is activated and the Python interpreter is set

**Step 3:** You're ready to start! The data loading code is already provided in this file.

**Note:** This challenge uses the same `index.qmd` file you're reading right now - you'll edit it to complete your analysis.
:::


### Getting Started Tips

::: {.callout-note}
## ðŸŽ¯ Method Chaining Philosophy

> "Each operation should build naturally on the previous one"

*Think of method chaining like building with LEGO blocks - each piece connects to the next, creating something more complex and useful than the individual pieces.*
:::

::: {.callout-warning}
## ðŸ’¾ Important: Save Your Work Frequently!

**Before you start:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After completing each mental model section
- After adding your visualizations
- After completing your advanced method chain
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::

## Grading Rubric ðŸŽ“

**75% Grade:** Complete discussion questions for at least 5 of the 7 mental models with clear, thoughtful responses.

**85% Grade:** Complete discussion questions for all 7 mental models with comprehensive, well-reasoned responses.

**95% Grade:** Complete all discussion questions plus the "Answering A Business Question" section.

**100% Grade:** Complete all discussion questions plus create a professional visualization showing service level by product category.

## Submission Checklist âœ…

**Minimum Requirements (Required for Any Points):**

- [ ] Created repository named "dataManipulationChallenge" in your GitHub account
- [ ] Cloned repository locally using Cursor (or VS Code)
- [ ] Completed discussion questions for at least 5 of the 7 mental models
- [ ] Document rendered to HTML successfully
- [ ] HTML files uploaded to your repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/dataManipulationChallenge/`

**75% Grade Requirements:**

- [ ] Complete discussion questions for at least 5 of the 7 mental models
- [ ] Clear, thoughtful responses that demonstrate understanding
- [ ] Code examples where appropriate to support explanations

**85% Grade Requirements:**

- [ ] Complete discussion questions for all 7 mental models
- [ ] Comprehensive, well-reasoned responses showing deep understanding
- [ ] Business context for why concepts matter
- [ ] Examples of real-world applications

**95% Grade Requirements:**

- [ ] Complete discussion questions for all 7 mental models
- [ ] Complete the "Answering A Business Question" discussion questions
- [ ] Comprehensive, well-reasoned responses showing deep understanding
- [ ] Business context for why concepts matter

**100% Grade Requirements:**

- [ ] All discussion questions completed with professional quality
- [ ] Professional visualization showing service level by product category
- [ ] Professional presentation style appropriate for business audience
- [ ] Clear, engaging narrative that tells a compelling story
- [ ] Practical insights that would help ZappTech's management

**Report Quality (Critical for Higher Grades):**

- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point


